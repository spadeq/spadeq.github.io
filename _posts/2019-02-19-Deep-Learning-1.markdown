---
layout: post
title: Deep Learning 笔记
date: 2019-02-19 11:11:00
categories: 
- DL
tags:
- Deep Learning
---

# M-P 神经元模型

真实的神经元具有树突和轴突。一个神经元（突触前细胞）的树突接受刺激后，通过神经递质，经轴突发送给另一个神经元（突触后细胞）。

M-P 神经元模型模拟神经元的工作模式。神经元体可以接受多个输入（模拟树突），并且为每个输入设置一个权重。那么在某个时刻，神经元体所接收到的信号可以加权求和：

$$\sum_iw_ix_i$$

当这个和超过某个预设的阈值 b 时，就产生输出（模拟轴突）：

$$f(\sum_iw_ix_i-b)$$

该模型的局限在于，这些 w 的权值就是神经元的参数，是事先确定的。

# 神经网络发展

* 1943：M-P 神经元模型
* 1956：感知机（Perceptron），可学习，w 权重可在学习过程中变化。悲惨的是，这种线性的模型连 XOR 运算都搞不定。
* 1986：分布式表示。从蚂蚁寻路中获取灵感，最终的复杂的神经网络，可能会由一些简单的小的局部模型组成。
* 1986：反向传播算法
* 1994：长短记忆网络
* 2006：深度信念网络
* 2007：卷积神经网络

06、07 年之前干不赢 SVM，从那之后，第三次兴起咸鱼翻身，改名深度学习。原因主要在于，第一现在有了「大」数据，样本数量级远超以往，第二现在有了「深」模型，计算机的算力大大提升（特别是 GPU 技术）。

# 前馈神经网络

从感知机中抽象出来的最简单的神经网络模型。

两个输入（权重分别为 $w_1$ 和 $w_2$），一个输出，

* 线性拟合：$y=w_1x_1+w_2x_2$
* Logistic 拟合：$y=\sigma(w_1x_1+w_2x_2)$。

最理想的响应函数是模拟大脑的阶跃函数，但是阶跃函数不可导（其实搞信号的应该知道求导后是冲激函数 $\delta(t)$），无法应用于梯度下降法。

## 深度神经网络

将神经元分层组合起来，每层的神经元之间没有连接，层与层之间进行互联。外侧的 input 和 output 两层之间，叫做隐藏层。当隐藏层数大于 2 的时候，就称为深度神经网络。

如果层次之间是单向传递数据的（没有反馈），就是前馈神经网络。

## 参数修正

网络的输出 $y$ 与理想输出 $\hat{y}$ 之间可能会产生差别。会有一系列算法（比如梯度下降），通过后反馈机制修正参数（权重）。

duang！TensorFlow 出场。Tensor 即张量，一维张量就是向量，二维张量就是矩阵。

# 卷积神经网络

各种功能的卷积核，在每一层执行卷积运算（离散形式的卷积和）形成输出送往下一层。神经网络中的卷积核也是从数据中**学习**到的。

# 循环神经网络

针对时序数据、序列化数据的处理。