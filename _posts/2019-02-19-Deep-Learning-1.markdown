---
layout: post
title: Deep Learning 笔记
date: 2019-02-23 15:43:00
categories: 
- DL
tags:
- Deep Learning
---

# M-P 神经元模型

真实的神经元具有树突和轴突。一个神经元（突触前细胞）的树突接受刺激后，通过神经递质，经轴突发送给另一个神经元（突触后细胞）。

M-P 神经元模型模拟神经元的工作模式。神经元体可以接受多个输入（模拟树突），并且为每个输入设置一个权重。那么在某个时刻，神经元体所接收到的信号可以加权求和：

$$\sum_iw_ix_i$$

当这个和超过某个预设的阈值 b 时，就产生输出（模拟轴突）：

$$f(\sum_iw_ix_i-b)$$

该模型的局限在于，这些 w 的权值就是神经元的参数，是事先确定的。

# 神经网络发展

* 1943：M-P 神经元模型
* 1956：感知机（Perceptron），可学习，w 权重可在学习过程中变化。悲惨的是，这种线性的模型连 XOR 运算都搞不定。
* 1986：分布式表示。从蚂蚁寻路中获取灵感，最终的复杂的神经网络，可能会由一些简单的小的局部模型组成。
* 1986：反向传播算法
* 1994：长短记忆网络
* 2006：深度信念网络
* 2007：卷积神经网络

06、07 年之前干不赢 SVM，从那之后，第三次兴起咸鱼翻身，改名深度学习。原因主要在于，第一现在有了「大」数据，样本数量级远超以往，第二现在有了「深」模型，计算机的算力大大提升（特别是 GPU 技术）。

# 前馈神经网络

从感知机中抽象出来的最简单的神经网络模型。

两个输入（权重分别为 $w_1$ 和 $w_2$），一个输出，

* 线性拟合：$y=w_1x_1+w_2x_2$
* Logistic 拟合：$y=\sigma(w_1x_1+w_2x_2)$。

最理想的响应函数是模拟大脑的阶跃函数，但是阶跃函数不可导（其实搞信号的应该知道求导后是冲激函数 $\delta(t)$），无法应用于梯度下降法。

## 深度神经网络

将神经元分层组合起来，每层的神经元之间没有连接，层与层之间进行互联。外侧的 input 和 output 两层之间，叫做隐藏层。当隐藏层数大于 2 的时候，就称为深度神经网络。

如果层次之间是单向传递数据的（没有反馈），就是前馈神经网络。

## 参数修正

网络的输出 $y$ 与理想输出 $\hat{y}$ 之间可能会产生差别。会有一系列算法（比如梯度下降），通过后反馈机制修正参数（权重）。

duang！TensorFlow 出场。Tensor 即张量，一维张量就是向量，二维张量就是矩阵。

# 卷积神经网络（CNN）

各种功能的卷积核，在每一层执行卷积运算（离散形式的卷积和）形成输出送往下一层。神经网络中的卷积核也是从数据中**学习**到的。

# 循环神经网络（RNN）

针对时序数据、序列化数据的处理。关键在于，隐藏层的函数会随着输入而变化，即 t 时刻的输入 x 所对应的隐藏层函数 h 实际上是由 t - 1 时刻的 x 和 h 来决定的。

降低参数数量，抓住时序数据中的长程。适用于自然语言处理。

面对的问题，是深度神经网络所共有的，叫做**梯度爆炸**。神经网络可以看作复合函数，梯度对应求导，复合函数求导存在着乘积。考虑权值 W，经过 100 多层后，如果 W 不等于 1，就会趋于无穷大或者 0。

## 长短记忆网络（LSTM）

引入「门」的概念。

* Input Gate（输入门）：决定放行多少输入，抑制掉不重要的输入；
* Memory Gate（记忆门）：决定记住多少信息；
* Output Gate（输出门）：决定多少信息会被释放出。

研究实验发现，输入门、输出门的实际作用很小，关键是中间的记忆门。所以，重新优化后仅保留记忆门，形成「门循环网络」（GRU）。

# 生产判别式网络（GAN）

判别式（Detective）对于图片进行判断，是真实的（Real）还是生成的（Generator）。生成式作为 Forger，可以将输入（往往是随机噪声）生成为贴近 Real 的图片。